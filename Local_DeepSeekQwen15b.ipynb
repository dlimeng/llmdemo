{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlimeng/llmdemo/blob/main/Local_DeepSeekQwen15b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrHRIznKp3nS"
      },
      "source": [
        "# Run LLM locally with vLLM on Colab GPUs\n",
        "\n",
        "Meng Li @Google AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0ZFs9rDvtJm"
      },
      "source": [
        "[vLLM](https://github.com/vllm-project/vllm) is a fast and user-friendly library for LLM inference and serving. vLLM optimizes LLM inference with mechanisms like PagedAttention for memory management and continuous batching for increasing throughput. For popular models, vLLM has been shown to increase throughput by a multiple of 2 to 4.\n",
        "\n",
        "This notebook demonstrates how to run machine learning inference by using vLLM and GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x41tnbTvQM1"
      },
      "source": [
        "## Requirements\n",
        "\n",
        "This notebook assumes that a GPU is enabled in Colab. If this setting isn't enabled, the locally executed sections of this notebook might not work. To enable a GPU, in the Colab menu, click **Runtime** > **Change runtime type**. For **Hardware accelerator**, choose a GPU accelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PSjyDIavRcn"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "Before creating your pipeline, download and install the dependencies required to develop with vLLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irCKNe42p22r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff6143e-089f-42c6-f791-63225d66fecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "ipython 7.34.0 requires jedi, which is not installed.\n",
            "pygobject 3.42.1 requires pycairo, which is not installed.\n"
          ]
        }
      ],
      "source": [
        "!pip install openai>=1.52.2\n",
        "!pip install vllm>=0.6.3\n",
        "!pip install triton>=3.1.0\n",
        "!pip install nest_asyncio # only needed in colab\n",
        "!pip check"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab only: allow nested asyncio\n",
        "\n",
        "The vLLM model handler logic below uses asyncio to feed vLLM records. This only works if we are not already in an asyncio event loop. Most of the time, this is fine, but colab already operates in an event loop. To work around this, we can use nest_asyncio to make things work smoothly in colab. Do not include this step outside of colab."
      ],
      "metadata": {
        "id": "3xz8zuA7vcS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This should not be necessary outside of colab.\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "sUqjOzw3wpI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUqjOzw3wpI4"
      },
      "source": [
        "## Run locally with vLLM\n",
        "\n",
        "In this section, you run a vLLM server. Use the `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model. This model is small enough to fit in Colab memory and doesn't require any extra authentication.\n",
        "\n",
        "First, start the vLLM server. This step might take a minute or two, because the model needs to download before vLLM starts running inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbJGzINNt5sG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f951825-8508-4824-f00a-4a53d068c039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-24 10:57:31.321653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-24 10:57:31.354484: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-24 10:57:31.364483: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-24 10:57:31.386886: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-24 10:57:32.997064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO 01-24 10:57:36 api_server.py:712] vLLM API server version 0.6.6.post1\n",
            "INFO 01-24 10:57:36 api_server.py:713] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='half', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\n",
            "INFO 01-24 10:57:36 api_server.py:199] Started engine process with PID 1460\n",
            "config.json: 100% 679/679 [00:00<00:00, 3.91MB/s]\n",
            "WARNING 01-24 10:57:36 config.py:2276] Casting torch.bfloat16 to torch.float16.\n",
            "2025-01-24 10:57:44.239550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-24 10:57:44.307849: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-24 10:57:44.325242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-24 10:57:47.564169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING 01-24 10:57:51 config.py:2276] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 01-24 10:57:56 config.py:510] This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\n",
            "WARNING 01-24 10:57:56 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
            "INFO 01-24 10:57:56 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "tokenizer_config.json: 100% 3.06k/3.06k [00:00<00:00, 21.7MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 38.2MB/s]\n",
            "INFO 01-24 10:58:05 config.py:510] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate', 'score'}. Defaulting to 'generate'.\n",
            "WARNING 01-24 10:58:05 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
            "INFO 01-24 10:58:05 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "INFO 01-24 10:58:05 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.47MB/s]\n",
            "INFO 01-24 10:58:06 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 01-24 10:58:06 selector.py:129] Using XFormers backend.\n",
            "INFO 01-24 10:58:07 model_runner.py:1094] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
            "INFO 01-24 10:58:08 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
            "model.safetensors: 100% 3.55G/3.55G [01:24<00:00, 42.1MB/s]\n",
            "INFO 01-24 10:59:32 weight_utils.py:296] No model.safetensors.index.json found in remote.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:07<00:00,  7.11s/it]\n",
            "INFO 01-24 10:59:40 model_runner.py:1099] Loading model weights took 3.3460 GB\n",
            "INFO 01-24 10:59:41 worker.py:241] Memory profiling takes 1.68 seconds\n",
            "INFO 01-24 10:59:41 worker.py:241] the current vLLM instance can use total_gpu_memory (14.75GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
            "INFO 01-24 10:59:41 worker.py:241] model weights take 3.35GiB; non_torch_memory takes 0.19GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.35GiB.\n",
            "INFO 01-24 10:59:42 gpu_executor.py:76] # GPU blocks: 19536, # CPU blocks: 9362\n",
            "INFO 01-24 10:59:42 gpu_executor.py:80] Maximum concurrency for 131072 tokens per request: 2.38x\n",
            "INFO 01-24 10:59:49 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "Capturing CUDA graph shapes: 100% 35/35 [00:30<00:00,  1.16it/s]\n",
            "INFO 01-24 11:00:19 model_runner.py:1535] Graph capturing finished in 30 secs, took 0.75 GiB\n",
            "INFO 01-24 11:00:19 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 39.41 seconds\n",
            "INFO 01-24 11:00:20 api_server.py:640] Using supplied chat template:\n",
            "INFO 01-24 11:00:20 api_server.py:640] None\n",
            "INFO 01-24 11:00:20 launcher.py:19] Available routes are:\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /health, Methods: GET\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /tokenize, Methods: POST\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /detokenize, Methods: POST\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /v1/models, Methods: GET\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /version, Methods: GET\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /v1/completions, Methods: POST\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /pooling, Methods: POST\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /score, Methods: POST\n",
            "INFO 01-24 11:00:20 launcher.py:27] Route: /v1/score, Methods: POST\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1370\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "INFO 01-24 11:04:44 logger.py:37] Received request cmpl-3de77e791d4c438ead6262dfa7686ac2-0: prompt: 'The meaning of life is to ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [151646, 785, 7290, 315, 2272, 374, 311, 220], lora_request: None, prompt_adapter_request: None.\n",
            "INFO 01-24 11:04:44 engine.py:267] Added request cmpl-3de77e791d4c438ead6262dfa7686ac2-0.\n",
            "INFO 01-24 11:04:50 metrics.py:467] Avg prompt throughput: 0.8 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:51280 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 01-24 11:05:00 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:05:10 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:05:24 logger.py:37] Received request cmpl-cc3e54017d074146b47c4d87329b7cad-0: prompt: 'Artificial intelligence means ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [151646, 9286, 16488, 11229, 3363, 220], lora_request: None, prompt_adapter_request: None.\n",
            "INFO 01-24 11:05:24 engine.py:267] Added request cmpl-cc3e54017d074146b47c4d87329b7cad-0.\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:53620 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 01-24 11:05:35 metrics.py:467] Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:05:45 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:05:48 logger.py:37] Received request cmpl-1c833b38f26e4122bead77911e07aaa8-0: prompt: 'A rainbow has ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [151646, 32, 47613, 702, 220], lora_request: None, prompt_adapter_request: None.\n",
            "INFO 01-24 11:05:48 engine.py:267] Added request cmpl-1c833b38f26e4122bead77911e07aaa8-0.\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:39868 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 01-24 11:05:58 metrics.py:467] Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:06:08 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:06:24 logger.py:37] Received request cmpl-f6df3facdd78459ba13cb21872957708-0: prompt: 'I am a ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [151646, 40, 1079, 264, 220], lora_request: None, prompt_adapter_request: None.\n",
            "INFO 01-24 11:06:24 engine.py:267] Added request cmpl-f6df3facdd78459ba13cb21872957708-0.\n",
            "INFO 01-24 11:06:24 metrics.py:467] Avg prompt throughput: 0.8 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:33936 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 01-24 11:06:35 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:06:45 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:07:55 logger.py:37] Received request cmpl-c758ebdefa3a406284f2f7e06a6874d0-0: prompt: 'The meaning of life is to ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [151646, 785, 7290, 315, 2272, 374, 311, 220], lora_request: None, prompt_adapter_request: None.\n",
            "INFO 01-24 11:07:55 engine.py:267] Added request cmpl-c758ebdefa3a406284f2f7e06a6874d0-0.\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:34438 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 01-24 11:08:05 metrics.py:467] Avg prompt throughput: 0.8 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:08:15 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:10:13 logger.py:37] Received request cmpl-7734705628aa49fd84cc6cfd129f0ceb-0: prompt: 'The meaning of life is to ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [151646, 785, 7290, 315, 2272, 374, 311, 220], lora_request: None, prompt_adapter_request: None.\n",
            "INFO 01-24 11:10:13 engine.py:267] Added request cmpl-7734705628aa49fd84cc6cfd129f0ceb-0.\n",
            "INFO 01-24 11:10:13 metrics.py:467] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:52810 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "INFO 01-24 11:10:23 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 01-24 11:10:33 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
          ]
        }
      ],
      "source": [
        "! python -m vllm.entrypoints.openai.api_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  --dtype half"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n35LXTS3uzIC"
      },
      "source": [
        "Next, while the vLLM server is running, open a separate terminal to communicate with the vLLM serving process. To open a terminal in Colab, in the sidebar, click **Terminal**. In the terminal, run the following commands.\n",
        "\n",
        "```\n",
        "curl http://localhost:8000/v1/completions \\\n",
        "    -H \"Content-Type: application/json\" \\\n",
        "    -d '{\n",
        "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"prompt\": \"The meaning of life is to \"\n",
        "    }'\n",
        "```\n",
        "\n",
        "This code runs against the server running in the cell. You can experiment with different prompts."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WipC2Q7U7sdn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
